Index: main.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/main.py b/main.py
--- a/main.py	(date 1618338627941)
+++ b/main.py	(date 1618338627941)
@@ -97,10 +97,11 @@
         self.last_weight = None
         self.last_bias = None
         self.tempOut = None
+        self.new_hidden = None
 
     def add_neuron(self):
-        self.hidden_list.append(torch.nn.Linear(self.iteration - 1 +
-                                                input_neurons, 1))
+        self.new_hidden = torch.nn.Linear(self.iteration - 1 +
+                                                input_neurons, 1)
         self.out = torch.nn.Linear(self.iteration + input_neurons, self.n_output)
         # self.out.param_group = "WDA"
         # print(self.out.param_group)
@@ -118,19 +119,27 @@
         """
         if iteration == 0:
             return self.out(x)
+        if iteration == 1:
+            ho = x
+            output = x
+            ho = self.new_hidden(output)
+            # print("in_feature: ", self.new_hidden.in_features)
+            output = torch.cat((output, ho), 1)
+            y_pred = self.out(output)
+            # print(x)
+            return y_pred
         else:
+            # print("in_feature: ", self.new_hidden.in_features, "iteration: ",self.iteration)
             ho = x
             output = x
             for hidden in self.hidden_list:
+                # print(hidden)
                 ho = hidden(output)
                 output = torch.cat((output, ho), 1)
-
-            # x = torch.cat((x, ho), 1)
-            # print("OUT")
             # print(output.size())
-            # print(self.out.in_features,self.out.out_features)
+            ho = self.new_hidden(output)
+            output = torch.cat((output, ho), 1)
             y_pred = self.out(output)
-            # print(x)
             return y_pred
 
 
@@ -145,15 +154,22 @@
 
 # store all losses for visualisation
 all_losses = []
-max_iter = 10
+max_iter = 5
 # print(net.parameters())
 # train a neural network
 for iteration in range(max_iter):
     # optimiser = torch.optim.Rprop(net.parameters(), lr=learning_rate)
-    optimiser = torch.optim.Rprop([
-        {"params": net.out.parameters(), "lr": learning_rate},
-        {"params": net.hidden_list.parameters(), "lr": learning_rate}
-    ], lr=learning_rate,)
+    if iteration > 0:
+        optimiser = torch.optim.Adam([
+            {"params": net.out.parameters(), "lr": learning_rate},
+            {"params": net.hidden_list.parameters(), "lr": 0},
+            {"params": net.new_hidden.parameters(), "lr": learning_rate}
+        ], lr=learning_rate,)
+    else:
+        optimiser = torch.optim.Adam([
+            {"params": net.out.parameters(), "lr": learning_rate},
+            {"params": net.hidden_list.parameters(), "lr": 0},
+        ], lr=learning_rate, )
     for epoch in range(num_epochs):
         # Perform forward pass: compute predicted y by passing x to the model.
         Y_pred = net(X)
@@ -187,9 +203,12 @@
         optimiser.step()
     # print("weight: " , net.out.weight)
     # print("bias: ", net.out.bias)
-    print(iteration,max_iter)
+    # print(iteration,max_iter)
     if iteration == max_iter - 1:
         break
+    if net.new_hidden is not None:
+        # print(net.new_hidden)
+        net.hidden_list.append(net.new_hidden)
     net.last_weight = net.out.weight
     net.last_bias = net.out.bias
     net.iteration += 1
